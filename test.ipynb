{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages.human import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\", base_url=\"http://127.0.0.1:11434\", keep_alive=-1)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"Translate this sentence from English to French: I love programming.\"\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(input1)\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2 = \"What did I just ask you?\"\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(input2)\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "print(demo_ephemeral_chat_history)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ephemeral_chat_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in demo_ephemeral_chat_history.messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        print(f\"Model: {message.response_metadata['model']}\")\n",
    "        print(f\"Response Time: {message.response_metadata['created_at']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history_dict = []\n",
    "\n",
    "# Iterate through the messages in the chat history\n",
    "for message in demo_ephemeral_chat_history.messages:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        # Append the human message to the list\n",
    "        message_history_dict.append({\"type\": \"Human\", \"content\": message.content})\n",
    "    elif isinstance(message, AIMessage):\n",
    "        # Append the AI message to the list\n",
    "        message_history_dict.append({\"type\": \"AI\", \"content\": message.content})\n",
    "\n",
    "# The resulting list of dictionaries\n",
    "print(message_history_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "history_dir = \"sessions\"\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store[session_id].messages\n",
    "type(store['abc2'].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "messages = []\n",
    "history = store['abc2']\n",
    "for msg in history.messages:\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        messages.append({\"role\": \"human\", \"content\": msg.content})\n",
    "    elif isinstance(msg, AIMessage):\n",
    "        messages.append({\"role\": \"ai\", \"content\": msg.content})\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    json.dump(messages, outfile)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.json\", \"r\") as readfile:\n",
    "    msg_hist = json.load(readfile)\n",
    "msg_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_hist[0]['content'])\n",
    "msg_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg_hist = ChatMessageHistory()\n",
    "\n",
    "for msg in msg_hist:\n",
    "    if msg['role'] == 'human':\n",
    "        ai_msg_hist.add_user_message(msg['content'])\n",
    "    elif msg['role'] == 'ai':\n",
    "        ai_msg_hist.add_ai_message(msg['content'])\n",
    "ai_msg_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Oh, I forgot it again! What was my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from session_handler import get_session_history\n",
    "\n",
    "ses_id = \"2\"\n",
    "msgs = get_session_history(ses_id)\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatbot_chain import get_chatbot_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from session_handler import get_session_history\n",
    "\n",
    "chatbot_chain = get_chatbot_chain()\n",
    "conversational_chatbot = RunnableWithMessageHistory(\n",
    "    chatbot_chain, get_session_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = conversational_chatbot.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob. What's yours?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversational_chatbot.invoke(\n",
    "    [HumanMessage(content=\"Hey Benedict, do you know why you are here?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "\n",
    "result = search.invoke(\"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults, DuckDuckGoSearchRun\n",
    "\n",
    "@tool\n",
    "def search_web(question):\n",
    "    \"\"\"\n",
    "    This function utilizes the DuckDuckGoSearchResults class to perform a web search based on \n",
    "    the input question. It invokes the search and returns the results obtained from DuckDuckGo.\n",
    "\n",
    "    Args:\n",
    "        question: The query string or question to search on the web from user.\n",
    "\n",
    "    Returns:\n",
    "        str: A str of search results retrieved from DuckDuckGo.\n",
    "        \n",
    "    Usage Context:\n",
    "        Use this tool to search the web for something you don't know about.\n",
    "    \"\"\"\n",
    "    search = DuckDuckGoSearchResults()\n",
    "    result = search.invoke(question)\n",
    "    \n",
    "    #return search\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Trump?\"\n",
    "result = search_web(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "web_search = DuckDuckGoSearchRun()\n",
    "web_tool = Tool(\n",
    "    name=\"web-search\",\n",
    "    description=\"Useful for when you need to do a search on the internet to find information that another tool can't find. be specific with your input.\",\n",
    "    func=web_search.run\n",
    ")\n",
    "tools = [web_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = []\n",
    "for tool in tools:\n",
    "    tool_names.append(tool.name)\n",
    "    print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt1 = hub.pull(\"hwchase17/react\")\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "agent_prompt = PromptTemplate.from_template(\"\"\"\n",
    "        Answer the following questions as best you can. You have access to the following tools:\n",
    "        {tools}\n",
    "\n",
    "        Use the following format:\n",
    "        \n",
    "        Question: the input question you must answer\n",
    "        Thought: you should always think about what to do\n",
    "        Action: the action to take, should be one of [{tool_names}]\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: the final answer to the original input question\n",
    "\n",
    "        Begin!\n",
    "\n",
    "        Question: {input}\n",
    "        Thought:{agent_scratchpad}\n",
    "    \"\"\")\n",
    "print(agent_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "def get_chatbot_chain():\n",
    "    agent_prompt = PromptTemplate.from_template(\"\"\"\n",
    "        Answer the following questions as best you can. You have access to the following tools:\n",
    "        {tools}\n",
    "\n",
    "        Use the following format:\n",
    "        \n",
    "        Question: the input question you must answer\n",
    "        Thought: you should always think about what to do\n",
    "        Action: the action to take, should be one of [{tool_names}]\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: the final answer to the original input question\n",
    "\n",
    "        Begin!\n",
    "\n",
    "        Question: {input}\n",
    "        Thought:{agent_scratchpad}\n",
    "    \"\"\")\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.1\", base_url=\"http://127.0.0.1:11434\", keep_alive=-1)\n",
    "    \n",
    "    agent = create_react_agent(llm, tools, agent_prompt)\n",
    "    \n",
    "    agent_exec = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=5, handle_parsing_errors=True)\n",
    "    \n",
    "    return agent_exec\n",
    "chatbot_chain = get_chatbot_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {\"input\": \"Hi, what's your name, how are you?\"}\n",
    "result = chatbot_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import ChatOllama\n",
    "def get_chatbot_chain():\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"\n",
    "                    Your name is Benedict. You are a helpful assistant with access to a web search tool. \n",
    "                    You can use this tool to search the web using DuckDuckGo. Answer all questions to the best of your ability.\n",
    "                    If you need additional information, use the web_tool to search for it.\n",
    "                \"\"\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.1\", base_url=\"http://127.0.0.1:11434\", keep_alive=-1)\n",
    "    \n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    chain = llm_with_tools | prompt\n",
    "    \n",
    "    return chain\n",
    "chatbot_chain = get_chatbot_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "variable messages should be a list of base messages, got content='' response_metadata={'model': 'llama3.1', 'created_at': '2024-08-08T16:09:47.8404144Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'web-search', 'arguments': {'config': {'args': ['search for name and current state in English'], 'kwargs': {}}, 'kwargs': {}}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 1853502000, 'load_duration': 18450200, 'prompt_eval_count': 194, 'prompt_eval_duration': 300532000, 'eval_count': 93, 'eval_duration': 1533474000} id='run-c2281ebb-6fa9-4753-8491-a7a2f7620ce8-0' tool_calls=[{'name': 'web-search', 'args': {'config': {'args': ['search for name and current state in English'], 'kwargs': {}}, 'kwargs': {}}, 'id': '07f45f59-e317-41cc-acb7-e2cfe3c6f9bd', 'type': 'tool_call'}] usage_metadata={'input_tokens': 194, 'output_tokens': 93, 'total_tokens': 287} of type <class 'langchain_core.messages.ai.AIMessage'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, what\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms your name, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:2875\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2873\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2874\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2875\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2876\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\prompts\\base.py:179\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    178\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1783\u001b[0m         Output,\n\u001b[1;32m-> 1784\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1785\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m             config,\n\u001b[0;32m   1789\u001b[0m             run_manager,\n\u001b[0;32m   1790\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1791\u001b[0m         ),\n\u001b[0;32m   1792\u001b[0m     )\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    427\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\prompts\\base.py:154\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    153\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\prompts\\chat.py:765\u001b[0m, in \u001b[0;36mBaseChatPromptTemplate.format_prompt\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m    757\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt. Should return a PromptValue.\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \n\u001b[0;32m    759\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m        PromptValue.\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 765\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mmessages)\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1206\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m   1204\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[0;32m   1205\u001b[0m ):\n\u001b[1;32m-> 1206\u001b[0m     message \u001b[38;5;241m=\u001b[39m message_template\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1207\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\radio\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_core\\prompts\\chat.py:231\u001b[0m, in \u001b[0;36mMessagesPlaceholder.format_messages\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    226\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name, [])\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptional\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name]\n\u001b[0;32m    229\u001b[0m )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be a list of base messages, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m value \u001b[38;5;241m=\u001b[39m convert_to_messages(value)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_messages:\n",
      "\u001b[1;31mValueError\u001b[0m: variable messages should be a list of base messages, got content='' response_metadata={'model': 'llama3.1', 'created_at': '2024-08-08T16:09:47.8404144Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'web-search', 'arguments': {'config': {'args': ['search for name and current state in English'], 'kwargs': {}}, 'kwargs': {}}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 1853502000, 'load_duration': 18450200, 'prompt_eval_count': 194, 'prompt_eval_duration': 300532000, 'eval_count': 93, 'eval_duration': 1533474000} id='run-c2281ebb-6fa9-4753-8491-a7a2f7620ce8-0' tool_calls=[{'name': 'web-search', 'args': {'config': {'args': ['search for name and current state in English'], 'kwargs': {}}, 'kwargs': {}}, 'id': '07f45f59-e317-41cc-acb7-e2cfe3c6f9bd', 'type': 'tool_call'}] usage_metadata={'input_tokens': 194, 'output_tokens': 93, 'total_tokens': 287} of type <class 'langchain_core.messages.ai.AIMessage'>"
     ]
    }
   ],
   "source": [
    "question = \"Hi, what's your name, how are you?\"\n",
    "result = chatbot_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
